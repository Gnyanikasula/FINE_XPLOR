{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import necessary libraries\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig\n)\n\nfrom huggingface_hub import login\nimport gradio as gr\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Hugging Face Login (with your Hugging Face token)\ntoken = \"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\"  # Replace with your Hugging Face token\nlogin(token)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Path to the fine-tuned model\nmodel_path = \"outputs_squad/merged_model\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load model function with quantization configuration\ndef load_model(model_name):\n    # Load model with 4-bit quantization\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=\"bfloat16\",\n        bnb_4bit_use_double_quant=True,\n    )\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        trust_remote_code=True,\n        device_map=\"auto\",\n        load_in_8bit=True,\n        quantization_config=bnb_config,\n        cache_dir=\"models\"\n    ).eval()\n\n    tokenizer = AutoTokenizer.from_pretrained(\n        model_name,\n        trust_remote_code=True\n    )\n    \n    # Inference class\n    class Infer:\n        def __init__(self, model, tokenizer):\n            self.model = model\n            self.tokenizer = tokenizer\n\n        def forward(self, text, limit=128, temp=1.0):\n            # Prepare input text\n            text = self.tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n            # Generate text using the model\n            output = self.model.generate(\n                **text,\n                do_sample=True,\n                temperature=temp,\n                max_new_tokens=int(limit),\n                top_p=0.95,\n                top_k=60,\n                pad_token_id=self.tokenizer.pad_token_id\n            )\n            # Decode output\n            return self.tokenizer.decode(output[0], skip_special_tokens=True)\n\n    return Infer(model, tokenizer)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load model\nmodel = load_model(model_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define prediction function\ndef predict(temp, limit, text):\n    prompt = text\n    # Use the inference class for prediction\n    out = model.forward(prompt, limit, temp)\n    return out\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create Gradio interface\npred = gr.Interface(\n    fn=predict,\n    inputs=[\n        gr.Slider(0.001, 10, value=0.1, label=\"Temperature\"),\n        gr.Slider(1, 1024, value=128, label=\"Token Limit\"),\n        gr.Textbox(\n            label=\"Input\",\n            lines=1,\n            value=\"#### Human: What's the capital of Australia?#### Assistant: \",\n        ),\n    ],\n    outputs='text',\n)\n# Launch Gradio app\npred.launch(share=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}